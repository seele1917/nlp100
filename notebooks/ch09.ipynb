{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章:  RNN, CNN\n",
    "https://nlp100.github.io/ja/ch09.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. ID番号への変換\n",
    "\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に`1`，2番目に頻出する単語に`2`，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて`0`とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import texthero as hero\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "df_train = pd.read_table('train.txt', header=None)\n",
    "title = hero.clean(df_train[1])\n",
    "words = sum(title.str.split().values, [])\n",
    "\n",
    "# ユニークな単語の個数を数えて，降順に並べてidxをidにしている\n",
    "count = pd.Series(words).value_counts()\n",
    "count = count[count>1] \n",
    "ids_dict = count.map({c:i+1 for i, c in enumerate(count.unique())})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[131, 129, 141, 124, 131, 139, 0, 135, 3, 158]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# wordsは配列かつlower適用済みを仮定\n",
    "def words2ids(words, ids_dict):\n",
    "    return [ids_dict.get(word, 0) for word in words]\n",
    "\n",
    "# テスト\n",
    "ids = words2ids(title[0].lower().split(), ids_dict)\n",
    "ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. RNNによる予測\n",
    "ID番号で表現された単語列$\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)$がある．ただし，$T$は単語列の長さ，$x_t \\in \\mathbb{R}^{V}$は単語のID番号のone-hot表記である（$V$は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリ$y$を予測するモデルとして，次式を実装せよ．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_0 = 0, \\\\\n",
    "\\overrightarrow{h}_t = {\\rm \\overrightarrow{RNN}}(\\mathrm{emb}(x_t), \\overrightarrow{h}_{t-1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} \\overrightarrow{h}_T + b^{(y)})\n",
    "$$\n",
    "\n",
    "ただし，$\\mathrm{emb}(x) \\in \\mathbb{R}^{d_w}$は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}$は時刻$t$の隠れ状態ベクトル，${\\rm \\overrightarrow{RNN}}(x,h)$は入力$x$と前時刻の隠れ状態$h$から次状態を計算するRNNユニット，$W^{(yh)} \\in \\mathbb{R}^{L \\times d_h}$は隠れ状態ベクトルからカテゴリを予測するための行列，$b^{(y)} \\in \\mathbb{R}^{L}$はバイアス項である（$d_w, d_h, L$)はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット${\\rm \\overrightarrow{RNN}}(x,h)$には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
    "\n",
    "$$\n",
    "{\\rm \\overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)})\n",
    "$$\n",
    "\n",
    "ただし，$W^{(hx)} \\in \\mathbb{R}^{d_h \\times d_w}，W^{(hh)} \\in \\mathbb{R}^{d_h \\times d_h}, b^{(h)} \\in \\mathbb{R}^{d_h}$はRNNユニットのパラメータ，$g$は活性化関数（例えばtanhやReLUなど）である．\n",
    "\n",
    "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでyを計算するだけでよい．次元数などのハイパーパラメータは，$d_w=300,d_h=50$など，適当な値に設定せよ（以降の問題でも同様である）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.7829,  0.9514,  0.2024, -0.7470]], grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, dw, dh, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, dw)\n",
    "        self.rnn = nn.RNN(dw, dh, batch_first=True)\n",
    "        self.linear = nn.Linear(dh, 4)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.emb(x)\n",
    "        y, _ = self.rnn(x, hidden)\n",
    "        y = self.linear(y[:, -1, :])\n",
    "        # y = self.softmax(y)\n",
    "        return y\n",
    "\n",
    "model = RNN(dw=300, dh=50, vocab_size=len(ids_dict))\n",
    "model(torch.tensor(ids)[None]) # テスト"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Xはpd.Seriesで文章を想定\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y=None, max_len=30):\n",
    "        self.X = hero.clean(X).str.split()\n",
    "        self.y = y\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = words2ids(self.X[index], ids_dict), self.y[index]\n",
    "        x = self.padding(ids)\n",
    "        return torch.tensor(x), torch.tensor(y).long()\n",
    "    \n",
    "    def padding(self , ids):\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [0]*(self.max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_len]\n",
    "        return ids\n",
    "\n",
    "def train_epoch(model, train_dataloader, criterion, optimizer, device):\n",
    "    model.to(device)\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    train_accuracy = 0\n",
    "    for X, T in train_dataloader:\n",
    "        X, T = X.to(device), T.to(device)\n",
    "        Y = model(X)\n",
    "        loss = criterion(Y, T)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred = Y.argmax(1)\n",
    "        train_loss += loss.item()*len(X)/len(train_dataloader.dataset)\n",
    "        train_accuracy += torch.sum(pred == T)/len(train_dataloader.dataset)\n",
    "    return train_loss, train_accuracy\n",
    "\n",
    "def valid_epoch(model, valid_dataloader, criterion, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    valid_loss = 0\n",
    "    valid_accuracy = 0\n",
    "    for X, T in valid_dataloader:\n",
    "        X, T = X.to(device), T.to(device)\n",
    "        with torch.no_grad():\n",
    "            Y = model(X)\n",
    "            loss = criterion(Y, T)\n",
    "        \n",
    "        pred = Y.argmax(1)\n",
    "        valid_loss += loss.item()*len(X)/len(valid_dataloader.dataset)\n",
    "        valid_accuracy += torch.sum(pred == T)/len(valid_dataloader.dataset)\n",
    "    return valid_loss, valid_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_dict = {\n",
    "    'b': 0,\n",
    "    't': 1,\n",
    "    'e': 2,\n",
    "    'm': 3,\n",
    "}\n",
    "\n",
    "def get_feature_and_label(txt_path):\n",
    "    df = pd.read_table(txt_path, header=None, names=['category', 'title'])\n",
    "    df['label'] = df['category'].map(category_dict)\n",
    "    return df['title'], df['label'].values\n",
    "\n",
    "X_train, y_train = get_feature_and_label('train.txt')\n",
    "X_valid, y_valid = get_feature_and_label('valid.txt')\n",
    "X_test, y_test = get_feature_and_label('test.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "\n",
    "dw, dh = 300, 50\n",
    "max_len = 100\n",
    "num_epochs = 5\n",
    "batch_size = 64\n",
    "lr = 1e-1\n",
    "\n",
    "model = RNN(dw=dw, dh=dh, vocab_size=len(ids_dict))\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "train_dataloader = DataLoader(Dataset(X_train, y_train, max_len=max_len), batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(Dataset(X_valid, y_valid, max_len=max_len), batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(Dataset(X_test, y_test, max_len=max_len), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 1.183, train_accuracy:  0.409. valid_loss:  1.205, valid_accuracy:  0.422\n",
      "epoch: 2, train_loss: 1.175, train_accuracy:  0.410. valid_loss:  1.163, valid_accuracy:  0.422\n",
      "epoch: 3, train_loss: 1.172, train_accuracy:  0.408. valid_loss:  1.175, valid_accuracy:  0.396\n",
      "epoch: 4, train_loss: 1.171, train_accuracy:  0.403. valid_loss:  1.167, valid_accuracy:  0.396\n",
      "epoch: 5, train_loss: 1.171, train_accuracy:  0.412. valid_loss:  1.170, valid_accuracy:  0.422\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_dataloader, criterion, device)\n",
    "    print(f'epoch: {epoch+1}, train_loss: {train_loss:.3f}, train_accuracy: {train_accuracy: .3f}. valid_loss: {valid_loss: .3f}, valid_accuracy: {valid_accuracy: .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，$B$事例ごとに損失・勾配を計算して学習を行えるようにせよ（$B$の値は適当に選べ）．また，GPU上で学習を実行せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1, train_loss: 1.169, train_accuracy:  0.402. valid_loss:  1.175, valid_accuracy:  0.396\n",
      "epoch: 2, train_loss: 1.167, train_accuracy:  0.412. valid_loss:  1.178, valid_accuracy:  0.396\n",
      "epoch: 3, train_loss: 1.167, train_accuracy:  0.407. valid_loss:  1.163, valid_accuracy:  0.422\n",
      "epoch: 4, train_loss: 1.166, train_accuracy:  0.414. valid_loss:  1.172, valid_accuracy:  0.396\n",
      "epoch: 5, train_loss: 1.166, train_accuracy:  0.416. valid_loss:  1.165, valid_accuracy:  0.396\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_dataloader = DataLoader(Dataset(X_train, y_train, max_len=max_len), batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(Dataset(X_valid, y_valid, max_len=max_len), batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(Dataset(X_test, y_test, max_len=max_len), batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_dataloader, criterion, device)\n",
    "    print(f'epoch: {epoch+1}, train_loss: {train_loss:.3f}, train_accuracy: {train_accuracy: .3f}. valid_loss: {valid_loss: .3f}, valid_accuracy: {valid_accuracy: .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, dw, dh, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, dw)\n",
    "        self.rnn = nn.RNN(dw, dh, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(dh*2, 4)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def init_emb(self, vectors, vocab_list):\n",
    "        for i, token in enumerate(vocab_list):\n",
    "            if token in vectors:\n",
    "                self.emb.weight.data[i] = torch.from_numpy(vectors[token])\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.emb(x)\n",
    "        y, hidden = self.rnn(x, hidden)\n",
    "        y = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "        y = self.linear(y[:, -1, :])\n",
    "        # y = self.softmax(y)\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "vectors = KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin.gz', binary=True)\n",
    "model = RNN(dw=dw, dh=dh, vocab_size=len(ids_dict))\n",
    "model.init_emb(vectors, words)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_dataloader, criterion, device)\n",
    "    print(f'epoch: {epoch+1}, train_loss: {train_loss:.3f}, train_accuracy: {train_accuracy: .3f}. valid_loss: {valid_loss: .3f}, valid_accuracy: {valid_accuracy: .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
    "\n",
    "$$\n",
    "\\overleftarrow{h}_{T+1} = 0, \\\\\n",
    "\\overleftarrow{h}_t = {\\rm \\overleftarrow{RNN}}(\\mathrm{emb}(x_t), \\overleftarrow{h}_{t+1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} [\\overrightarrow{h}_T; \\overleftarrow{h}_1] + b^{(y)})\n",
    "$$\n",
    "\n",
    "ただし，$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}, \\overleftarrow{h}_t \\in \\mathbb{R}^{d_h}$はそれぞれ，順方向および逆方向のRNNで求めた時刻$t$の隠れ状態ベクトル，${\\rm \\overleftarrow{RNN}}(x,h)$は入力$x$と次時刻の隠れ状態$h$から前状態を計算するRNNユニット，$W^{(yh)} \\in \\mathbb{R}^{L \\times 2d_h}$は隠れ状態ベクトルからカテゴリを予測するための行列，$b^{(y)} \\in \\mathbb{R}^{L}$はバイアス項である．また，$[a;b]$はベクトル$a$と$b$の連結を表す。\n",
    "\n",
    "さらに，双方向RNNを多層化して実験せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, dw, dh, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, dw)\n",
    "        self.rnn = nn.RNN(dw, dh, batch_first=True, bidirectional=True)\n",
    "        self.linear = nn.Linear(dh*2, 4)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, x, hidden=None):\n",
    "        x = self.emb(x)\n",
    "        y, hidden = self.rnn(x, hidden)\n",
    "        y = torch.cat([hidden[0], hidden[1]], dim=1)\n",
    "        y = self.linear(y[:, -1, :])\n",
    "        # y = self.softmax(y)\n",
    "        return y\n",
    "\n",
    "model = RNN(dw=dw, dh=dh, vocab_size=len(ids_dict))\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_dataloader, criterion, device)\n",
    "    print(f'epoch: {epoch+1}, train_loss: {train_loss:.3f}, train_accuracy: {train_accuracy: .3f}. valid_loss: {valid_loss: .3f}, valid_accuracy: {valid_accuracy: .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列$\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)$がある．ただし，$T$は単語列の長さ，$x_t \\in \\mathbb{R}^{V}$は単語のID番号のone-hot表記である（$V$は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列$\\boldsymbol{x}$からカテゴリ$y$を予測するモデルを実装せよ．\n",
    "\n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "\n",
    "- 単語埋め込みの次元数: $d_w$\n",
    "- 畳み込みのフィルターのサイズ: 3 トークン\n",
    "- 畳み込みのストライド: 1 トークン\n",
    "- 畳み込みのパディング: あり\n",
    "- 畳み込み演算後の各時刻のベクトルの次元数: $d_h$\n",
    "- 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を$d_h$次元の隠れベクトルで表現\n",
    "\n",
    "すなわち，時刻$t$の特徴ベクトル$pt∈ℝdh$は次式で表される．\n",
    "\n",
    "$$\n",
    "p_t = g(W^{(px)} [\\mathrm{emb}(x_{t-1}); \\mathrm{emb}(x_t); \\mathrm{emb}(x_{t+1})] + b^{(p)})\n",
    "$$\n",
    "\n",
    "ただし，$W^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h}$はCNNのパラメータ，$g$は活性化関数（例えばtanhやReLUなど），$[a;b;c]$はベクトル$a,b,c$の連結である．なお，行列$W^{(px)}$の列数が3$d_w$になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
    "\n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル$c \\in \\mathbb{R}^{d_h}$を求める．$c[i]$でベクトル$c$の$i$番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "\n",
    "$$\n",
    "c[i] = \\max_{1 \\leq t \\leq T} p_t[i]\n",
    "$$\n",
    "\n",
    "最後に，入力文書の特徴ベクトル$c$に行列$W^{(yc)} \\in \\mathbb{R}^{L \\times d_h}$とバイアス項$b^{(y)}\\in \\mathbb{R}^L$による線形変換とソフトマックス関数を適用し，カテゴリ$y$を予測する．\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$y$を計算するだけでよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, dw, dh, vocab_size):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size+1, dw)\n",
    "        self.conv = nn.Conv1d(dw, dh, 3, padding=1) \n",
    "        self.relu = nn.ReLU()\n",
    "        self.pool = nn.MaxPool1d(max_len)\n",
    "        self.linear = nn.Linear(dh, 4)\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)\n",
    "        x = x.view(x.shape[0], x.shape[2], x.shape[1])\n",
    "        x = self.conv(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.view(x.shape[0], x.shape[1], x.shape[2])\n",
    "        x = self.pool(x)\n",
    "        x = x.view(x.shape[0], x.shape[1])\n",
    "        y = self.linear(x)\n",
    "        # y = self.softmax(y)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "model = CNN(dw=dw, dh=dh, vocab_size=len(ids_dict))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_dataloader, criterion, device)\n",
    "    print(f'epoch: {epoch+1}, train_loss: {train_loss:.3f}, train_accuracy: {train_accuracy: .3f}. valid_loss: {valid_loss: .3f}, valid_accuracy: {valid_accuracy: .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adamに変更\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "model = CNN(dw=dw, dh=dh, vocab_size=len(ids_dict))\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_dataloader, criterion, device)\n",
    "    print(f'epoch: {epoch+1}, train_loss: {train_loss:.3f}, train_accuracy: {train_accuracy: .3f}. valid_loss: {valid_loss: .3f}, valid_accuracy: {valid_accuracy: .3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "class BERT(nn.Module):\n",
    "    def __init__(self, n_unit=768, n_classes=4):\n",
    "        super().__init__()\n",
    "        self.n_unit = n_unit\n",
    "        self.bert_model = BertModel.from_pretrained('bert-base-uncased') \n",
    "        self.fc = nn.Linear(n_unit, n_classes)\n",
    "\n",
    "    def forward(self, ids):\n",
    "        seg_ids = torch.zeros_like(ids)\n",
    "        attention_mask = (ids > 0)\n",
    "        last_hidden_state = self.bert_model(input_ids=ids, token_type_ids=seg_ids, attention_mask=attention_mask).last_hidden_state\n",
    "        x = last_hidden_state[:, 0, :]\n",
    "        logit = self.fc(x.view(-1, self.n_unit))\n",
    "        return logit\n",
    "\n",
    "\n",
    "# Xはpd.Seriesで文章を想定\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y=None, max_len=30, tokenizer=None):\n",
    "        self.X = hero.clean(X)\n",
    "        self.y = y\n",
    "        self.max_len = max_len\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        x, y = tokenizer.encode(self.X[index], max_length=512), self.y[index]\n",
    "        x = self.padding(x)\n",
    "        return torch.tensor(x), torch.tensor(y).long()\n",
    "\n",
    "    def padding(self , ids):\n",
    "        if len(ids) < self.max_len:\n",
    "            ids += [0]*(self.max_len - len(ids))\n",
    "        else:\n",
    "            ids = ids[:self.max_len]\n",
    "        return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "train_dataloader = DataLoader(Dataset(X_train, y_train, max_len=max_len, tokenizer=tokenizer), batch_size=batch_size, shuffle=True)\n",
    "valid_dataloader = DataLoader(Dataset(X_valid, y_valid, max_len=max_len, tokenizer=tokenizer), batch_size=batch_size, shuffle=False)\n",
    "test_dataloader = DataLoader(Dataset(X_test, y_test, max_len=max_len, tokenizer=tokenizer), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "model = BERT()\n",
    "\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "    else:\n",
    "        param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_accuracy = train_epoch(model, train_dataloader, criterion, optimizer, device)\n",
    "    valid_loss, valid_accuracy = valid_epoch(model, valid_dataloader, criterion, device)\n",
    "    print(f'epoch: {epoch+1}, train_loss: {train_loss:.3f}, train_accuracy: {train_accuracy: .3f}. valid_loss: {valid_loss: .3f}, valid_accuracy: {valid_accuracy: .3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
