{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第9章:  RNN, CNN\n",
    "https://nlp100.github.io/ja/ch09.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80. ID番号への変換\n",
    "\n",
    "問題51で構築した学習データ中の単語にユニークなID番号を付与したい．学習データ中で最も頻出する単語に`1`，2番目に頻出する単語に`2`，……といった方法で，学習データ中で2回以上出現する単語にID番号を付与せよ．そして，与えられた単語列に対して，ID番号の列を返す関数を実装せよ．ただし，出現頻度が2回未満の単語のID番号はすべて`0`とせよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 81. RNNによる予測\n",
    "ID番号で表現された単語列$\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)$がある．ただし，$T$は単語列の長さ，$x_t \\in \\mathbb{R}^{V}$は単語のID番号のone-hot表記である（$V$は単語の総数である）．再帰型ニューラルネットワーク（RNN: Recurrent Neural Network）を用い，単語列xからカテゴリ$y$を予測するモデルとして，次式を実装せよ．\n",
    "\n",
    "$$\n",
    "\\overrightarrow{h}_0 = 0, \\\\\n",
    "\\overrightarrow{h}_t = {\\rm \\overrightarrow{RNN}}(\\mathrm{emb}(x_t), \\overrightarrow{h}_{t-1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} \\overrightarrow{h}_T + b^{(y)})\n",
    "$$\n",
    "\n",
    "ただし，$\\mathrm{emb}(x) \\in \\mathbb{R}^{d_w}$は単語埋め込み（単語のone-hot表記から単語ベクトルに変換する関数），$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}$は時刻$t$の隠れ状態ベクトル，${\\rm \\overrightarrow{RNN}}(x,h)$は入力$x$と前時刻の隠れ状態$h$から次状態を計算するRNNユニット，$W^{(yh)} \\in \\mathbb{R}^{L \\times d_h}$は隠れ状態ベクトルからカテゴリを予測するための行列，$b^{(y)} \\in \\mathbb{R}^{L}$はバイアス項である（$d_w, d_h, L$)はそれぞれ，単語埋め込みの次元数，隠れ状態ベクトルの次元数，ラベル数である）．RNNユニット${\\rm \\overrightarrow{RNN}}(x,h)$には様々な構成が考えられるが，典型例として次式が挙げられる．\n",
    "\n",
    "$$\n",
    "{\\rm \\overrightarrow{RNN}}(x,h) = g(W^{(hx)} x + W^{(hh)}h + b^{(h)})\n",
    "$$\n",
    "\n",
    "ただし，$W^{(hx)} \\in \\mathbb{R}^{d_h \\times d_w}，W^{(hh)} \\in \\mathbb{R}^{d_h \\times d_h}, b^{(h)} \\in \\mathbb{R}^{d_h}$はRNNユニットのパラメータ，$g$は活性化関数（例えばtanhやReLUなど）である．\n",
    "\n",
    "なお，この問題ではパラメータの学習を行わず，ランダムに初期化されたパラメータでyを計算するだけでよい．次元数などのハイパーパラメータは，$d_w=300,d_h=50$など，適当な値に設定せよ（以降の問題でも同様である）．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 82. 確率的勾配降下法による学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題81で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 83. ミニバッチ化・GPU上での学習\n",
    "問題82のコードを改変し，$B$事例ごとに損失・勾配を計算して学習を行えるようにせよ（$B$の値は適当に選べ）．また，GPU上で学習を実行せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 84. 単語ベクトルの導入\n",
    "事前学習済みの単語ベクトル（例えば，Google Newsデータセット（約1,000億単語）での[学習済み単語ベクトル](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit?usp=sharing)）で単語埋め込みemb(x)を初期化し，学習せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 85. 双方向RNN・多層化\n",
    "順方向と逆方向のRNNの両方を用いて入力テキストをエンコードし，モデルを学習せよ．\n",
    "\n",
    "$$\n",
    "\\overleftarrow{h}_{T+1} = 0, \\\\\n",
    "\\overleftarrow{h}_t = {\\rm \\overleftarrow{RNN}}(\\mathrm{emb}(x_t), \\overleftarrow{h}_{t+1}), \\\\\n",
    "y = {\\rm softmax}(W^{(yh)} [\\overrightarrow{h}_T; \\overleftarrow{h}_1] + b^{(y)})\n",
    "$$\n",
    "\n",
    "ただし，$\\overrightarrow{h}_t \\in \\mathbb{R}^{d_h}, \\overleftarrow{h}_t \\in \\mathbb{R}^{d_h}$はそれぞれ，順方向および逆方向のRNNで求めた時刻$t$の隠れ状態ベクトル，${\\rm \\overleftarrow{RNN}}(x,h)$は入力$x$と次時刻の隠れ状態$h$から前状態を計算するRNNユニット，$W^{(yh)} \\in \\mathbb{R}^{L \\times 2d_h}$は隠れ状態ベクトルからカテゴリを予測するための行列，$b^{(y)} \\in \\mathbb{R}^{L}$はバイアス項である．また，$[a;b]$はベクトル$a$と$b$の連結を表す。\n",
    "\n",
    "さらに，双方向RNNを多層化して実験せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 86. 畳み込みニューラルネットワーク (CNN)\n",
    "ID番号で表現された単語列$\\boldsymbol{x} = (x_1, x_2, \\dots, x_T)$がある．ただし，$T$は単語列の長さ，$x_t \\in \\mathbb{R}^{V}$は単語のID番号のone-hot表記である（$V$は単語の総数である）．畳み込みニューラルネットワーク（CNN: Convolutional Neural Network）を用い，単語列$\\boldsymbol{x}$からカテゴリ$y$を予測するモデルを実装せよ．\n",
    "\n",
    "ただし，畳み込みニューラルネットワークの構成は以下の通りとする．\n",
    "\n",
    "- 単語埋め込みの次元数: $d_w$\n",
    "- 畳み込みのフィルターのサイズ: 3 トークン\n",
    "- 畳み込みのストライド: 1 トークン\n",
    "- 畳み込みのパディング: あり\n",
    "- 畳み込み演算後の各時刻のベクトルの次元数: $d_h$\n",
    "- 畳み込み演算後に最大値プーリング（max pooling）を適用し，入力文を$d_h$次元の隠れベクトルで表現\n",
    "\n",
    "すなわち，時刻$t$の特徴ベクトル$pt∈ℝdh$は次式で表される．\n",
    "\n",
    "$$\n",
    "p_t = g(W^{(px)} [\\mathrm{emb}(x_{t-1}); \\mathrm{emb}(x_t); \\mathrm{emb}(x_{t+1})] + b^{(p)})\n",
    "$$\n",
    "\n",
    "ただし，$W^{(px)} \\in \\mathbb{R}^{d_h \\times 3d_w}, b^{(p)} \\in \\mathbb{R}^{d_h}$はCNNのパラメータ，$g$は活性化関数（例えばtanhやReLUなど），$[a;b;c]$はベクトル$a,b,c$の連結である．なお，行列$W^{(px)}$の列数が3$d_w$になるのは，3個のトークンの単語埋め込みを連結したものに対して，線形変換を行うためである．\n",
    "\n",
    "最大値プーリングでは，特徴ベクトルの次元毎に全時刻における最大値を取り，入力文書の特徴ベクトル$c \\in \\mathbb{R}^{d_h}$を求める．$c[i]$でベクトル$c$の$i$番目の次元の値を表すことにすると，最大値プーリングは次式で表される．\n",
    "\n",
    "$$\n",
    "c[i] = \\max_{1 \\leq t \\leq T} p_t[i]\n",
    "$$\n",
    "\n",
    "最後に，入力文書の特徴ベクトル$c$に行列$W^{(yc)} \\in \\mathbb{R}^{L \\times d_h}$とバイアス項$b^{(y)}\\in \\mathbb{R}^L$による線形変換とソフトマックス関数を適用し，カテゴリ$y$を予測する．\n",
    "なお，この問題ではモデルの学習を行わず，ランダムに初期化された重み行列で$y$を計算するだけでよい．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 87. 確率的勾配降下法によるCNNの学習\n",
    "確率的勾配降下法（SGD: Stochastic Gradient Descent）を用いて，問題86で構築したモデルを学習せよ．訓練データ上の損失と正解率，評価データ上の損失と正解率を表示しながらモデルを学習し，適当な基準（例えば10エポックなど）で終了させよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 88. パラメータチューニング\n",
    "問題85や問題87のコードを改変し，ニューラルネットワークの形状やハイパーパラメータを調整しながら，高性能なカテゴリ分類器を構築せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 89. 事前学習済み言語モデルからの転移学習\n",
    "事前学習済み言語モデル（例えばBERTなど）を出発点として，ニュース記事見出しをカテゴリに分類するモデルを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
