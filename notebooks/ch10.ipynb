{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第10章:  機械翻訳\n",
    "\n",
    "本章では，日本語と英語の翻訳コーパスである京都フリー翻訳タスク (KFTT)を用い，ニューラル機械翻訳モデルを構築する．ニューラル機械翻訳モデルの構築には，fairseq，Hugging Face Transformers，OpenNMT-pyなどの既存のツールを活用せよ.\n",
    "\n",
    "https://nlp100.github.io/ja/ch10.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 90. データの準備\n",
    "機械翻訳のデータセットをダウンロードせよ．訓練データ，開発データ，評価データを整形し，必要に応じてトークン化などの前処理を行うこと．ただし，この段階ではトークンの単位として形態素（日本語）および単語（英語）を採用せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget http://www.phontron.com/kftt/download/kftt-data-1.0.tar.gz\n",
    "# !tar zxvf kftt-data-1.0.tar.gz\n",
    "\n",
    "!cat kftt-data-1.0/data/orig/kyoto-train.ja | sed 's/\\s+/ /g' | ginzame > train.ginza.ja\n",
    "!cat kftt-data-1.0/data/orig/kyoto-dev.ja | sed 's/\\s+/ /g' | ginzame > dev.ginza.ja\n",
    "!cat kftt-data-1.0/data/orig/kyoto-test.ja | sed 's/\\s+/ /g' | ginzame > test.ginza.ja"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/bin/bash: gizname: command not found\n"
     ]
    }
   ],
   "source": [
    "for src, dst in [ ('train.ginza.ja', 'train.spacy.ja'), ('dev.ginza.ja', 'dev.spacy.ja'), ('test.ginza.ja', 'test.spacy.ja')]:\n",
    "    with open(src) as f:\n",
    "        lst = []\n",
    "        tmp = []\n",
    "        for x in f:\n",
    "            x = x.strip()\n",
    "            if x == 'EOS':\n",
    "                lst.append(' '.join(tmp))\n",
    "                tmp = []\n",
    "            elif x != '':\n",
    "                tmp.append(x.split('\\t')[0])\n",
    "    with open(dst, 'w') as f:\n",
    "        for line in lst:\n",
    "            print(line, file=f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 91. 機械翻訳モデルの訓練\n",
    "90で準備したデータを用いて，ニューラル機械翻訳のモデルを学習せよ（ニューラルネットワークのモデルはTransformerやLSTMなど適当に選んでよい）．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess -s ja -t en \\\n",
    "    --trainpref train.spacy \\\n",
    "    --validpref dev.spacy \\\n",
    "    --destdir data91  \\\n",
    "    --thresholdsrc 5 \\\n",
    "    --thresholdtgt 5 \\\n",
    "    --workers 20\n",
    "\n",
    "!fairseq-train data91 \\\n",
    "    --fp16 \\\n",
    "    --save-dir save91 \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 91.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 92. 機械翻訳モデルの適用\n",
    "91で学習したニューラル機械翻訳モデルを用い，与えられた（任意の）日本語の文を英語に翻訳するプログラムを実装せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-interactive --path save91/checkpoint10.pt data91 < test.spacy.ja | grep '^H' | cut -f3 > 92.out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 93. BLEUスコアの計測\n",
    "91で学習したニューラル機械翻訳モデルの品質を調べるため，評価データにおけるBLEUスコアを測定せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-score --sys 92.out --ref test.spacy.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 94. ビーム探索\n",
    "91で学習したニューラル機械翻訳モデルで翻訳文をデコードする際に，ビーム探索を導入せよ．ビーム幅を1から100くらいまで適当に変化させながら，開発セット上のBLEUスコアの変化をプロットせよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "for N in `seq 1 20` ; do\n",
    "    fairseq-interactive --path save91/checkpoint10.pt --beam $N data91 < test.spacy.ja | grep '^H' | cut -f3 > 94.$N.out\n",
    "done\n",
    "\n",
    "for N in `seq 1 20` ; do\n",
    "    fairseq-score --sys 94.$N.out --ref test.spacy.en > 94.$N.score\n",
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def read_score(filename):\n",
    "    with open(filename) as f:\n",
    "        x = f.readlines()[1]\n",
    "        x = re.search(r'(?<=BLEU4 = )\\d*\\.\\d*(?=,)', x)\n",
    "        return float(x.group())\n",
    "\n",
    "xs = range(1, 21)\n",
    "ys = [read_score(f'94.{x}.score') for x in xs]\n",
    "plt.plot(xs, ys)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 95. サブワード化\n",
    "トークンの単位を単語や形態素からサブワードに変更し，91-94の実験を再度実施せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm\n",
    "\n",
    "spm.SentencePieceTrainer.Train('--input=kftt-data-1.0/data/orig/kyoto-train.ja --model_prefix=kyoto_ja --vocab_size=16000 --character_coverage=1.0')\n",
    "\n",
    "sp = spm.SentencePieceProcessor()\n",
    "sp.Load('kyoto_ja.model')\n",
    "\n",
    "for src, dst in [\n",
    "    ('kftt-data-1.0/data/orig/kyoto-train.ja', 'train.sub.ja'),\n",
    "    ('kftt-data-1.0/data/orig/kyoto-dev.ja', 'dev.sub.ja'),\n",
    "    ('kftt-data-1.0/data/orig/kyoto-test.ja', 'test.sub.ja'),\n",
    "]:\n",
    "    with open(src) as f, open(dst, 'w') as g:\n",
    "        for x in f:\n",
    "            x = x.strip()\n",
    "            x = re.sub(r'\\s+', ' ', x)\n",
    "            x = sp.encode_as_pieces(x)\n",
    "            x = ' '.join(x)\n",
    "            print(x, file=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!subword-nmt learn-bpe -s 16000 < kftt-data-1.0/data/orig/kyoto-train.en > kyoto_en.codes\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-train.en > train.sub.en\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-dev.en > dev.sub.en\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < kftt-data-1.0/data/orig/kyoto-test.en > test.sub.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "fairseq-preprocess -s ja -t en \\\n",
    "    --trainpref train.sub \\\n",
    "    --validpref dev.sub \\\n",
    "    --destdir data95  \\\n",
    "    --workers 20\n",
    "\n",
    "fairseq-train data95 \\\n",
    "    --fp16 \\\n",
    "    --save-dir save95 \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --update-freq 1 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 95.log\n",
    "\n",
    "fairseq-interactive --path save95/checkpoint10.pt data95 < test.sub.ja | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > 95.out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenize(src, dst):\n",
    "    with open(src) as f, open(dst, 'w') as g:\n",
    "        for x in f:\n",
    "            x = x.strip()\n",
    "            x = ' '.join([doc.text for doc in nlp(x)])\n",
    "            print(x, file=g)\n",
    "spacy_tokenize('95.out', '95.out.spacy')\n",
    "!fairseq-score --sys 95.out.spacy --ref test.spacy.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 96. 学習過程の可視化\n",
    "Tensorboardなどのツールを用い，ニューラル機械翻訳モデルが学習されていく過程を可視化せよ．可視化する項目としては，学習データにおける損失関数の値とBLEUスコア，開発データにおける損失関数の値とBLEUスコアなどを採用せよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-train data95 \\\n",
    "    --fp16 \\\n",
    "    --tensorboard-logdir log96 \\\n",
    "    --save-dir save96 \\\n",
    "    --max-epoch 5 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.2 --weight-decay 0.0001 \\\n",
    "    --update-freq 1 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 96.log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 97. ハイパー・パラメータの調整\n",
    "ニューラルネットワークのモデルや，そのハイパーパラメータを変更しつつ，開発データにおけるBLEUスコアが最大となるモデルとハイパーパラメータを求めよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-train data95 \\\n",
    "    --fp16 \\\n",
    "    --save-dir save97_1 \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.1 --weight-decay 0.0001 \\\n",
    "    --update-freq 1 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 97_1.log\n",
    "\n",
    "!fairseq-train data95 \\\n",
    "    --fp16 \\\n",
    "    --save-dir save97_3 \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.3 --weight-decay 0.0001 \\\n",
    "    --update-freq 1 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 97_3.log\n",
    "\n",
    "!fairseq-train data95 \\\n",
    "    --fp16 \\\n",
    "    --save-dir save97_5 \\\n",
    "    --max-epoch 10 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-3 --lr-scheduler inverse_sqrt --warmup-updates 2000 \\\n",
    "    --dropout 0.5 --weight-decay 0.0001 \\\n",
    "    --update-freq 1 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 97_5.log\n",
    "\n",
    "!fairseq-interactive --path save97_1/checkpoint10.pt data95 < test.sub.ja | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > 97_1.out\n",
    "!fairseq-interactive --path save97_3/checkpoint10.pt data95 < test.sub.ja | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > 97_3.out\n",
    "!fairseq-interactive --path save97_5/checkpoint10.pt data95 < test.sub.ja | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > 97_5.out\n",
    "\n",
    "spacy_tokenize('97_1.out', '97_1.out.spacy')\n",
    "spacy_tokenize('97_3.out', '97_3.out.spacy')\n",
    "spacy_tokenize('97_5.out', '97_5.out.spacy')\n",
    "\n",
    "!fairseq-score --sys 97_1.out.spacy --ref test.spacy.en\n",
    "!fairseq-score --sys 97_3.out.spacy --ref test.spacy.en\n",
    "!fairseq-score --sys 97_5.out.spacy --ref test.spacy.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 98. ドメイン適応\n",
    "Japanese-English Subtitle Corpus (JESC)やJParaCrawlなどの翻訳データを活用し，KFTTのテストデータの性能向上を試みよ．\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open('en-ja.tar.gz') as tar:\n",
    "    for f in tar.getmembers():\n",
    "        if f.name.endswith('txt'):\n",
    "            text = tar.extractfile(f).read().decode('utf-8')\n",
    "            break\n",
    "\n",
    "data = text.splitlines()\n",
    "data = [x.split('\\t') for x in data]\n",
    "data = [x for x in data if len(x) == 4]\n",
    "data = [[x[3], x[2]] for x in data]\n",
    "\n",
    "with open('jparacrawl.ja', 'w') as f, open('jparacrawl.en', 'w') as g:\n",
    "    for j, e in data:\n",
    "        print(j, file=f)\n",
    "        print(e, file=g)\n",
    "\n",
    "\n",
    "with open('jparacrawl.ja') as f, open('train.jparacrawl.ja', 'w') as g:\n",
    "    for x in f:\n",
    "        x = x.strip()\n",
    "        x = re.sub(r'\\s+', ' ', x)\n",
    "        x = sp.encode_as_pieces(x)\n",
    "        x = ' '.join(x)\n",
    "        print(x, file=g)\n",
    "\n",
    "!subword-nmt apply-bpe -c kyoto_en.codes < jparacrawl.en > train.jparacrawl.en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!fairseq-preprocess -s ja -t en \\\n",
    "    --trainpref train.jparacrawl \\\n",
    "    --validpref dev.sub \\\n",
    "    --destdir data98  \\\n",
    "    --workers 20\n",
    "\n",
    "!fairseq-train data98 \\\n",
    "    --fp16 \\\n",
    "    --save-dir save98_1 \\\n",
    "    --max-epoch 3 \\\n",
    "    --arch transformer --share-decoder-input-output-embed \\\n",
    "    --optimizer adam --clip-norm 1.0 \\\n",
    "    --lr 1e-4 --lr-scheduler inverse_sqrt --warmup-updates 4000 \\\n",
    "    --dropout 0.1 --weight-decay 0.0001 \\\n",
    "    --criterion label_smoothed_cross_entropy --label-smoothing 0.1 \\\n",
    "    --max-tokens 8000 > 98_1.log\n",
    "\n",
    "!fairseq-interactive --path save98_1/checkpoint3.pt data98 < test.sub.ja | grep '^H' | cut -f3 | sed -r 's/(@@ )|(@@ ?$)//g' > 98_1.out\n",
    "\n",
    "spacy_tokenize('98_1.out', '98_1.out.spacy')\n",
    "\n",
    "!fairseq-score --sys 98_1.out.spacy --ref test.spacy.en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 99. 翻訳サーバの構築\n",
    "ユーザが翻訳したい文を入力すると，その翻訳結果がウェブブラウザ上で表示されるデモシステムを構築せよ．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
